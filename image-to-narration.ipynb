{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport re\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom collections import Counter\nfrom PIL import Image\nfrom collections import Counter\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.models as models\nimport torchvision.transforms as T\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader, Dataset\n\nimport spacy\nfrom nltk.corpus import stopwords","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T18:30:47.177698Z","iopub.execute_input":"2025-02-15T18:30:47.178003Z","iopub.status.idle":"2025-02-15T18:30:47.182717Z","shell.execute_reply.started":"2025-02-15T18:30:47.177982Z","shell.execute_reply":"2025-02-15T18:30:47.181990Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"Data Source: https://www.kaggle.com/datasets/adityajn105/flickr8k","metadata":{}},{"cell_type":"code","source":"data_location = '../input/flickr8k/'\ndf = pd.read_csv(data_location + \"captions.txt\")\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T18:30:51.858995Z","iopub.execute_input":"2025-02-15T18:30:51.859291Z","iopub.status.idle":"2025-02-15T18:30:51.918495Z","shell.execute_reply.started":"2025-02-15T18:30:51.859266Z","shell.execute_reply":"2025-02-15T18:30:51.917640Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"                       image  \\\n0  1000268201_693b08cb0e.jpg   \n1  1000268201_693b08cb0e.jpg   \n2  1000268201_693b08cb0e.jpg   \n3  1000268201_693b08cb0e.jpg   \n4  1000268201_693b08cb0e.jpg   \n\n                                             caption  \n0  A child in a pink dress is climbing up a set o...  \n1              A girl going into a wooden building .  \n2   A little girl climbing into a wooden playhouse .  \n3  A little girl climbing the stairs to her playh...  \n4  A little girl in a pink dress going into a woo...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>caption</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1000268201_693b08cb0e.jpg</td>\n      <td>A child in a pink dress is climbing up a set o...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1000268201_693b08cb0e.jpg</td>\n      <td>A girl going into a wooden building .</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1000268201_693b08cb0e.jpg</td>\n      <td>A little girl climbing into a wooden playhouse .</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1000268201_693b08cb0e.jpg</td>\n      <td>A little girl climbing the stairs to her playh...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1000268201_693b08cb0e.jpg</td>\n      <td>A little girl in a pink dress going into a woo...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"data_idx = 100\nimage_path = data_location+\"/Images/\"+df.iloc[data_idx,0]\nprint(image_path)\nimg=mpimg.imread(image_path)\nplt.imshow(img)\nplt.show()","metadata":{"trusted":true},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-ab0680a89434>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'n' is not defined"],"ename":"NameError","evalue":"name 'n' is not defined","output_type":"error"}],"execution_count":9},{"cell_type":"code","source":"spacy_eng = spacy.load('en_core_web_sm')\ntext = 'This is a good place for study'\n#Test Code\n[token.text.lower() for token in spacy_eng.tokenizer(text)]\nfor token in spacy_eng(text):\n    print(token)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T18:29:43.279502Z","iopub.execute_input":"2025-02-15T18:29:43.279839Z","iopub.status.idle":"2025-02-15T18:29:43.900339Z","shell.execute_reply.started":"2025-02-15T18:29:43.279814Z","shell.execute_reply":"2025-02-15T18:29:43.899509Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n  warnings.warn(Warnings.W111)\n","output_type":"stream"},{"name":"stdout","text":"This\nis\na\ngood\nplace\nfor\nstudy\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"def preprocess_string(s):\n    # Remove all non-word characters (everything except numbers and letters)\n    s = re.sub(r\"[^\\w\\s]\", '', s)\n    # Replace all runs of whitespaces with no space\n    s = re.sub(r\"\\s+\", '', s)\n    # replace digits with no space\n    s = re.sub(r\"\\d\", '', s)\n    return s\n\ndef show_image(inp, title = None):\n    inp = inp.numpy().transpose((1, 2, 0)) # For showing we need to transpose it(channel, height, width)\n    plt.imshow(inp)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T18:31:07.300348Z","iopub.execute_input":"2025-02-15T18:31:07.300640Z","iopub.status.idle":"2025-02-15T18:31:07.305341Z","shell.execute_reply.started":"2025-02-15T18:31:07.300620Z","shell.execute_reply":"2025-02-15T18:31:07.304463Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def tokenizer(text):\n    word_list = []\n    for word in text.lower().split():\n        word = preprocess_string(word)\n        word_list.append(word)\n    return word_list","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T06:03:01.962601Z","iopub.execute_input":"2025-02-15T06:03:01.962843Z","iopub.status.idle":"2025-02-15T06:03:01.975704Z","shell.execute_reply.started":"2025-02-15T06:03:01.962818Z","shell.execute_reply":"2025-02-15T06:03:01.974808Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Vocabulary:\n    def __init__(self, freq_threshold):\n        self.freq_threshold = freq_threshold\n        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n        self.stoi = {v: k for k, v in self.itos.items()}\n\n    def __len__(self):\n        return len(self.itos)\n\n    @staticmethod\n    def tokenize(text):\n        return tokenizer(text)\n\n    def build_vocab(self, sentence_list):\n        freq = Counter()\n        idx = len(self.itos)\n        \n        for sentence in sentence_list:\n            for word in self.tokenize(sentence):\n                freq[word] += 1\n            \n        for word, count in freq.items():\n            if count >= self.freq_threshold: \n                self.itos[idx] = word\n                idx += 1\n\n    def numericalized(self, text):\n        tokenized_text = self.tokenize(text)\n        numericalized_text = []\n        for token in tokenized_text:\n            numericalized_text.append(self.stoi.get(token, self.stoi[\"<UNK>\"])) \n        return numericalized_text\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T18:33:59.196445Z","iopub.execute_input":"2025-02-15T18:33:59.196797Z","iopub.status.idle":"2025-02-15T18:33:59.203052Z","shell.execute_reply.started":"2025-02-15T18:33:59.196769Z","shell.execute_reply":"2025-02-15T18:33:59.202069Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"class FlickrDataset(Dataset):\n    def __init__(self, root_dir, captions_file, transform=None,freq_threshold=5):\n        self.root_dir = root_dir\n        self.df = pd.read_csv(captions_file)\n        self.transform = transform\n        self.imgs = self.df[\"image\"]\n        self.captions = self.df[\"caption\"]\n        self.vocab = Vocabulary(freq_threshold)\n        self.vocab.build_vocab(self.captions.tolist())\n    def __len__(self):\n        return len(self.df)\n\n\n    # def __getitem__(self, idx):\n    #     caption = self.captions[idx]\n    #     img_name = self.imgs[idx]\n    #     img_location = os.path.join(self.root_dir,img_name)\n    #     img = Image.open(img_location).convert(\"RGB\")\n    #     caption_vec = []\n    #     caption_vec += [self.vocab.stoi[\"<SOS>\"]]\n    #     caption_vec += self.vocab.numericalize(caption)\n    #     caption_vec += [self.vocab.stoi[\"<EOS>\"]]\n    #     return img, torch.tensor(caption_vec)\n\n    def __getitem__(self, idx):\n        caption = self.captions[idx]\n        img_name = self.imgs[idx]\n        img_location = os.path.join(self.root_dir, img_name)\n        \n        # Open the image\n        img = Image.open(img_location).convert(\"RGB\")\n    \n        # Apply transformations\n        if self.transform:\n            img = self.transform(img)\n    \n        caption_vec = [self.vocab.stoi[\"<SOS>\"]]\n        caption_vec += self.vocab.numericalized(caption)\n        caption_vec += [self.vocab.stoi[\"<EOS>\"]]\n        return img, torch.tensor(caption_vec)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T18:34:00.089015Z","iopub.execute_input":"2025-02-15T18:34:00.089304Z","iopub.status.idle":"2025-02-15T18:34:00.095281Z","shell.execute_reply.started":"2025-02-15T18:34:00.089280Z","shell.execute_reply":"2025-02-15T18:34:00.094434Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"transforms = T.Compose([\n    T.Resize((64, 64), interpolation=Image.LANCZOS),\n    T.ToTensor()\n])\nprint('========>>')\ndataset =  FlickrDataset(\n    root_dir = data_location+\"/Images\",\n    captions_file = data_location+\"/captions.txt\",\n    transform=transforms\n)\nimg, caps = dataset[35]\nshow_image(img,\"Image\")\nprint(\"Token:\",caps)\nprint(\"Sentence:\")\n# for token in caps.tolist():\n#     result.append(dataset.vocab.itos[token])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T06:03:02.013347Z","iopub.execute_input":"2025-02-15T06:03:02.013626Z","iopub.status.idle":"2025-02-15T06:03:03.669611Z","shell.execute_reply.started":"2025-02-15T06:03:02.013604Z","shell.execute_reply":"2025-02-15T06:03:03.668862Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# class CapsCollate:\n\n#     def __init__(self,pad_idx,batch_first=False):\n#         self.pad_idx = pad_idx\n#         self.batch_first = batch_first\n\n#     def __call__(self,batch):\n        \n#         imgs = torch.cat(imgs,dim=0)\n#         targets = [item[1] for item in batch]\n#         targets = pad_sequence(targets, batch_first=self.batch_first, padding_value=self.pad_idx)\n#         return imgs,targets\n\n\nimport torch\nfrom torch.nn.utils.rnn import pad_sequence\n\nclass CapsCollate:\n    def __init__(self, pad_idx, batch_first=False):\n        self.pad_idx = pad_idx\n        self.batch_first = batch_first\n\n    def __call__(self, batch):\n        # Extract images and captions from the batch\n        imgs = [item[0].unsqueeze(0) for item in batch]  # Unsqueeze to add batch dim\n        targets = [item[1] for item in batch]  # Captions (already tensor)\n\n        # Stack images into a single tensor\n        imgs = torch.cat(imgs, dim=0)  # Concatenate along batch dimension\n\n        # Pad captions to match the longest caption in the batch\n        targets = pad_sequence(targets, batch_first=self.batch_first, padding_value=self.pad_idx)\n\n        return imgs, targets\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T06:03:03.670579Z","iopub.execute_input":"2025-02-15T06:03:03.670937Z","iopub.status.idle":"2025-02-15T06:03:03.676915Z","shell.execute_reply.started":"2025-02-15T06:03:03.670900Z","shell.execute_reply":"2025-02-15T06:03:03.675956Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#for image plot\nimport matplotlib.pyplot as plt\ndef show_image(img, title=None):\n    \"\"\"Imshow for Tensor.\"\"\"\n\n    #unnormalize\n    img[0] = img[0] * 0.229\n    img[1] = img[1] * 0.224\n    img[2] = img[2] * 0.225\n    img[0] += 0.485\n    img[1] += 0.456\n    img[2] += 0.406\n\n    img = img.numpy().transpose((1, 2, 0))\n\n\n    plt.imshow(img)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001)  # pause a bit so that plots are updated","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T06:03:03.677747Z","iopub.execute_input":"2025-02-15T06:03:03.678032Z","iopub.status.idle":"2025-02-15T06:03:03.697426Z","shell.execute_reply.started":"2025-02-15T06:03:03.677996Z","shell.execute_reply":"2025-02-15T06:03:03.696601Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#writing the dataloader\ndata_location =  \"../input/flickr8k\"\nBATCH_SIZE = 256\nNUM_WORKER = 2\npad_idx = dataset.vocab.stoi[\"<PAD>\"]\ndata_loader = DataLoader(\n    dataset=dataset,\n    batch_size=BATCH_SIZE,\n    num_workers=NUM_WORKER,\n    shuffle=True,\n    collate_fn=CapsCollate(pad_idx=pad_idx,batch_first=True)\n)\n\n\n\n#generating the iterator from the dataloader\ndataiter = iter(data_loader)\n\n#getting the next batch\nbatch = next(dataiter)\n\n#unpacking the batch\nimages, captions = batch\n\n#showing info of image in single batch\n# for i in range(BATCH_SIZE):\n#     img,cap = images[i],captions[i]\n#     caption_label = [dataset.vocab.itos[token] for token in cap.tolist()]\n#     eos_index = caption_label.index('<EOS>')\n#     caption_label = caption_label[1:eos_index]\n#     caption_label = ' '.join(caption_label)\n#     show_image(img,caption_label)\n#     plt.show()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T06:03:03.698290Z","iopub.execute_input":"2025-02-15T06:03:03.698546Z","iopub.status.idle":"2025-02-15T06:03:07.062714Z","shell.execute_reply.started":"2025-02-15T06:03:03.698513Z","shell.execute_reply":"2025-02-15T06:03:07.060917Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#testing the vicab class\nv = Vocabulary(freq_threshold=1)\n\nv.build_vocab([\"This is a good place to find a city\"])\nprint(v.stoi)\nprint(v.numericalized(\"This is a good place to find a city here!!\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T06:03:07.064642Z","iopub.execute_input":"2025-02-15T06:03:07.065124Z","iopub.status.idle":"2025-02-15T06:03:07.072964Z","shell.execute_reply.started":"2025-02-15T06:03:07.065076Z","shell.execute_reply":"2025-02-15T06:03:07.071728Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#setting the constants\n\n#defining the transform to be applied\ntransforms = T.Compose([\n    T.Resize(256),\n    T.RandomCrop(224),\n    T.ToTensor(),\n    T.Normalize((0.485, 0.456, 0.406),(0.229, 0.224, 0.225))\n])\n\n\n#testing the dataset class\ndataset =  FlickrDataset(\n    root_dir = data_location+\"/Images\",\n    captions_file = data_location+\"/captions.txt\",\n    transform=transforms\n)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T06:03:07.073854Z","iopub.execute_input":"2025-02-15T06:03:07.074168Z","iopub.status.idle":"2025-02-15T06:03:09.109596Z","shell.execute_reply.started":"2025-02-15T06:03:07.074145Z","shell.execute_reply":"2025-02-15T06:03:09.108560Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class EncoderCNN(nn.Module):\n    def __init__(self, embed_size):\n        #super(EncoderCNN, self).__init__()\n        super().__init__()\n        resnet = models.resnet50(pretrained=True)\n        for param in resnet.parameters():\n            param.requires_grad_(False)\n\n        modules = list(resnet.children())[:-1]\n        self.resnet = nn.Sequential(*modules)\n        self.linear = nn.Linear(resnet.fc.in_features, embed_size)\n\n    def forward(self, images):\n        features = self.resnet(images)\n        features = features.view(features.size(0), -1)\n        features = self.linear(features)\n        return features\n\nclass DecoderRNN(nn.Module):\n    def __init__(self,embed_size, hidden_size, vocab_size, num_layers=1,drop_prob=0.3):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size,embed_size)\n        self.lstm = nn.LSTM(embed_size,hidden_size,num_layers=num_layers,batch_first=True)\n        self.fcn = nn.Linear(hidden_size,vocab_size)\n        self.drop = nn.Dropout(drop_prob)\n\n    def forward(self, features, captions):\n\n        #vectorize the caption\n        embeds = self.embedding(captions[:,:-1])\n\n        #concat the features and captions\n        x = torch.cat((features.unsqueeze(1),embeds),dim=1)\n        x,_ = self.lstm(x)\n        x = self.fcn(x)\n        return x\n\n    def generate_caption(self,inputs,hidden=None,max_len=20,vocab=None):\n        # Inference part\n        # Given the image features generate the captions\n\n        batch_size = inputs.size(0)\n\n        captions = []\n\n        for i in range(max_len):\n            output,hidden = self.lstm(inputs,hidden)\n            output = self.fcn(output)\n            output = output.view(batch_size,-1)\n\n\n            #select the word with most val\n            predicted_word_idx = output.argmax(dim=1)\n\n            #save the generated word\n            captions.append(predicted_word_idx.item())\n\n            #end if <EOS detected>\n            if vocab.itos[predicted_word_idx.item()] == \"<EOS>\":\n                break\n\n            #send generated word as the next caption\n            inputs = self.embedding(predicted_word_idx.unsqueeze(0))\n\n        #covert the vocab idx to words and return sentence\n        return [vocab.itos[idx] for idx in captions]\n\n\nclass EncoderDecoder(nn.Module):\n    def __init__(self,embed_size, hidden_size, vocab_size, num_layers=1,drop_prob=0.3):\n        super().__init__()\n        self.encoder = EncoderCNN(embed_size)\n        self.decoder = DecoderRNN(embed_size,hidden_size,vocab_size,num_layers,drop_prob)\n\n    def forward(self, images, captions):\n        features = self.encoder(images)\n        outputs = self.decoder(features, captions)\n        return outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T06:03:09.110843Z","iopub.execute_input":"2025-02-15T06:03:09.111196Z","iopub.status.idle":"2025-02-15T06:03:09.124534Z","shell.execute_reply.started":"2025-02-15T06:03:09.111165Z","shell.execute_reply":"2025-02-15T06:03:09.123386Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T06:03:09.125444Z","iopub.execute_input":"2025-02-15T06:03:09.125764Z","iopub.status.idle":"2025-02-15T06:03:09.156464Z","shell.execute_reply.started":"2025-02-15T06:03:09.125734Z","shell.execute_reply":"2025-02-15T06:03:09.155569Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Hyperparameters\nembed_size = 400\nhidden_size = 512\nvocab_size = len(dataset.vocab)\nnum_layers = 2\nlearning_rate = 0.0001\nnum_epochs = 2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T06:03:09.157679Z","iopub.execute_input":"2025-02-15T06:03:09.158067Z","iopub.status.idle":"2025-02-15T06:03:09.175862Z","shell.execute_reply.started":"2025-02-15T06:03:09.158031Z","shell.execute_reply":"2025-02-15T06:03:09.175150Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# initialize model, loss etc\nmodel = EncoderDecoder(embed_size, hidden_size, vocab_size, num_layers).to(device)\ncriterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T06:03:09.176731Z","iopub.execute_input":"2025-02-15T06:03:09.177038Z","iopub.status.idle":"2025-02-15T06:03:10.071018Z","shell.execute_reply.started":"2025-02-15T06:03:09.176975Z","shell.execute_reply":"2025-02-15T06:03:10.070148Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm import tqdm\n\nnum_epochs = 20\nprint_every = 30\n\nfor epoch in range(1,num_epochs+1):\n    idx = 0\n    for image, captions in tqdm(data_loader):\n        image,captions = image.to(device),captions.to(device)\n\n        # Zero the gradients.\n        optimizer.zero_grad()\n\n        # Feed forward\n        outputs = model(image, captions)\n\n        # Calculate the batch loss.\n        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n\n\n        # Backward pass.\n        loss.backward()\n\n        # Update the parameters in the optimizer.\n        optimizer.step()\n\n        if (idx+1)%print_every == 0:\n            print(\"Epoch: {} loss: {:.5f}\".format(epoch,loss.item()))\n\n\n            #generate the caption\n            model.eval()\n            with torch.no_grad():\n                dataiter = iter(data_loader)\n                img,_ = next(dataiter)\n                features = model.encoder(img[0:1].to(device))\n                caps = model.decoder.generate_caption(features.unsqueeze(0),vocab=dataset.vocab)\n                caption = ' '.join(caps)\n                show_image(img[0],title=caption)\n\n            model.train()\n\n        idx += 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T06:03:10.072154Z","iopub.execute_input":"2025-02-15T06:03:10.072526Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
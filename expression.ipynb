{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1351797,"sourceType":"datasetVersion","datasetId":786787},{"sourceId":62448586,"sourceType":"kernelVersion"}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Assignment on Facial Expression Recognition with CNN\nThe task in this assignment is to train a convolutional neural network that can recognize the facial expression from an image.\n\n## Dataset\nYou will use the FER2013 dataset. The dataset is available in `torchvision`. For more details, see [here](https://pytorch.org/vision/main/generated/torchvision.datasets.FER2013.html#torchvision.datasets.FER2013).\n\n## Architectures\nIn this assignment, you will experiment with the following architectures.\n* Alexnet\n* VGG11\n* Resnet18\n* Inception V3\n\nMake necessary adjustments to the architecture to adapt it for you classification task. Do not use pre-trained weights.\n\n## Training\n* Train each model for several epochs\n* Track the training loss and training accuracies as well as the testing loss and testing accuracies at each epoch (Note: This is only for simplicity. We typically do not use test set during the training).\n*  Tune batch size and learning rates hyper-parameters. Experiment with at least 3 values of the hyper-parameter.\n\n## Submission Guidelines\n* Plot the training loss and test loss separately for each configuration (architecture, batch size, learning rate).\n* Comment on the learning curves.\n* Submit the following by 21 February, 2025\n    * A PDF containing the learning curves and the discussions.\n    * A `zip` file containing your codes.","metadata":{"id":"nPdiPxlKxdax"}},{"cell_type":"code","source":"import os\nimport re\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom collections import Counter\nfrom PIL import Image\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.models as models\nimport torchvision.transforms as T\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import datasets\n\nimport spacy\nfrom nltk.corpus import stopwords\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transform = T.Compose([\n    T.Grayscale(num_output_channels=3),\n    T.Resize((224, 224)),  \n    T.ToTensor(),         \n    T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  ])\ndata_location_train =  '../input/fer2013/train/'\ndata_location_test =  '../input/fer2013/test/'\n\ntrain_dataset = datasets.ImageFolder(root= data_location_train, transform=transform)\ntest_dataset = datasets.ImageFolder(root=data_location_test, transform=transform)","metadata":{"trusted":true,"id":"stpj364qxda1"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Classes:\", train_dataset.classes) \n# def imshow(img):\n#     img = img / 2 + 0.5      \n#     npimg = img.numpy()\n#     plt.imshow(np.transpose(npimg, (1, 2, 0)))\n#     plt.show()\n\n# dataiter = iter(train_data_loader)\n# images, labels = next(dataiter)\n# imshow(images[0])\n# print(train_dataset.classes[labels[0]])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The following function will return train and test data loaders with a given batch size.","metadata":{"id":"mnjvPZM7xda2"}},{"cell_type":"code","source":"def get_data_loaders(batch_size):\n    train_data_loader = DataLoader(train_dataset, batch_size= batch_size, shuffle=False)\n    test_data_loader = DataLoader(test_dataset, batch_size= batch_size, shuffle=False)\n    return train_data_loader, test_data_loader","metadata":{"trusted":true,"id":"fUza-RvCxda3"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The following function will return an instance of the model that will be trained. Here `name` will be one of the following, `[\"alexnet\", \"vgg11\", \"resnet\", \"inception\"]`.","metadata":{"id":"BIAbVFqzxda3"}},{"cell_type":"code","source":"def get_model(model_name, num_classes):\n    if model_name == \"alexnet\":\n        model = models.alexnet(num_classes=num_classes, weights=None)\n\n    elif model_name == \"vgg11\":\n        model = models.vgg11(num_classes=num_classes, weights=None)\n\n    elif model_name == \"resnet18\":\n        model = models.resnet18(num_classes=num_classes, weights=None)\n\n    elif model_name == \"inceptionv3\":\n        model = models.inception_v3(num_classes=num_classes, weights=None)\n        \n    return model","metadata":{"trusted":true,"id":"SXHntnW-xda3"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The following functions will return the training criterion and optimizer respectively.","metadata":{"id":"K5YQ_5ptxda4"}},{"cell_type":"code","source":"def get_criterion():\n    criterion = nn.CrossEntropyLoss()\n    return criterion\n\ndef get_optimizer(model, learning_rate):\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n    return optimizer","metadata":{"trusted":true,"id":"In2i5Z5Dxda4"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The following function will plot the training and test loss/accuracy vs epoch. You can use the `what` and `title` parameters to detect which learning curve is being plot. If you want, you can also save the plots for furture use.","metadata":{"id":"Oo9gE_5axda5"}},{"cell_type":"markdown","source":"The following function will train the model using the train loader and assess its performance on both the train and test data.","metadata":{"id":"HQSrnHZAxda4"}},{"cell_type":"code","source":"def train(model, train_loader, test_loader, num_epochs, optimizer, criterion):\n    model.to(device)\n    train_accuracy_list = []\n    train_loss_list = []\n    test_accuracy_list = []\n    test_loss_list = []\n\n    for epoch in range(num_epochs):\n        model.train()\n        total = 0\n        correct = 0\n        train_loss = 0\n        \n        for img, labels in train_loader:\n            img, labels = img.to(device), labels.to(device)\n            optimizer.zero_grad()  # Add missing zero_grad() to reset gradients\n            outputs = model(img)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            train_loss += loss.item()\n            _, pred = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (pred == labels).sum().item()\n    \n        train_acc = 100 * correct / total\n        train_accuracy_list.append(train_acc)\n        train_loss_list.append(train_loss / len(train_loader))\n\n        model.eval()\n        correct = 0\n        total = 0\n        test_loss = 0\n        \n        with torch.no_grad():\n            for images, labels in test_loader:\n                images, labels = images.to(device), labels.to(device)\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n                test_loss += loss.item()\n                _, predicted = torch.max(outputs, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n    \n        test_acc = 100 * correct / total\n        test_accuracy_list.append(test_acc)\n        test_loss_list.append(test_loss / len(test_loader))\n                            \n        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss_list[-1]:.4f}, Test Loss: {test_loss_list[-1]:.4f}, Train Acc: {train_acc:.2f}%, Test Acc: {test_acc:.2f}%\")\n        \n    return train_accuracy_list, train_loss_list, test_accuracy_list, test_loss_list\n","metadata":{"trusted":true,"id":"zuwe3iLexda5"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot(train_values, test_values, what, title):\n    plt.plot(train_values, label='Train')\n    plt.plot(test_values, label='Test')\n    plt.xlabel('Epochs')\n    plt.ylabel(what)\n    plt.title(title)\n    plt.legend()\n    plt.show()","metadata":{"trusted":true,"id":"27pIxoIlxda5"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nbatch_sizes = [32, 64,128]\nlearning_rates = [0.01, 0.001,0.001]\nnames = ['alexnet', 'vgg11', 'resnet18', 'inceptionv3']\n# modify the value in the following statement\nnum_epocs = 10\nnum_classes = 7\nfor bs in batch_sizes:\n    for lr in learning_rates:\n        for name in names:\n            print(f\"==========Training {name} with batch size {bs} and learning rate {lr}========\")\n            train_loader, test_loader = get_data_loaders(bs)\n            model = get_model(name, num_classes)\n            optimizer = get_optimizer(model,lr)\n            criterion = get_criterion()\n\n            train_acc, train_loss, test_acc, test_loss = train(\n                model, train_loader, test_loader, num_epocs, optimizer, criterion)\n\n            plot(train_acc, test_acc, what='acc', title=f'{name}_{bs}_{lr}_{\"acc\"}')\n            plot(train_loss, test_loss, what='loss', title=f'{name}_{bs}_{lr}_{\"loss\"}')","metadata":{"trusted":true,"id":"scC4AiuAxda5"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}